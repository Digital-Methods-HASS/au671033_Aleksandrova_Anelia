---
title: 'Text mining, sentiment analysis, and visualization'
date: 'created on 22 November 2020 and updated `r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```


### Get the Game of thrones text:
```{r get-document}
ipcc_path <- here("data","got.pdf")
ipcc_text <- pdf_text(ipcc_path)
#ipcc_text returns a vector of strings, one for each page of the book GOT
```


```{r single-page}
ipcc_p9 <- ipcc_text[9] #getting a single page. Pdtools adds \n and a\
ipcc_p9
```

### Some wrangling:

- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r split-lines}
ipcc_df <- data.frame(ipcc_text) %>% 
  mutate(text_full = str_split(ipcc_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 
head(ipcc_df)

```


### Get the tokens (individual words) in tidy format

```{r tokenize}
ipcc_tokens <- ipcc_df %>% 
  unnest_tokens(word, text_full)

```

Count the words!
```{r count-words}
ipcc_wc <- ipcc_tokens %>% 
  count(word) %>% 
  arrange(-n)
ipcc_wc
```

### Remove stop words:

```{r stopwords}
ipcc_stop <- ipcc_tokens %>% 
  anti_join(stop_words) %>% 
  select(-ipcc_text)
```

Check the counts again: 
```{r count-words2}
ipcc_swc <- ipcc_stop %>% 
  count(word) %>% 
  arrange(-n)
```

What if we want to get rid of all the numbers (non-text) in `ipcc_stop`?
```{r skip-numbers}
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed

ipcc_no_numeric <- ipcc_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of Game of thrones book words 


```{r wordcloud-prep}
# There are almost 2000 unique words 
length(unique(ipcc_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
ipcc_top100 <- ipcc_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
ipcc_cloud <- ggplot(data = ipcc_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

ipcc_cloud
```

Customizing the word cloud:
```{r wordcloud-pro}
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```



### Sentiment analysis

```{r afinn}
get_sentiments(lexicon = "afinn")


# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos
```

bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```

nrc:https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm
Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative. 

**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```

Let's do sentiment analysis on the Game og thrones data using afinn, and nrc. 


### Sentiment analysis with afinn: 

First, bind words in `ipcc_stop` to `afinn` lexicon:
```{r bind-afinn}
ipcc_afinn <- ipcc_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
ipcc_afinn_hist <- ipcc_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = ipcc_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```

Investigate some of the words in a bit more depth:
```{r afinn-2}

ipcc_afinn2 <- ipcc_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(ipcc_afinn2$word)

# Count & plot them
ipcc_afinn2_n <- ipcc_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = ipcc_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

```


Summarizing sentiment for the report: 
```{r summarize-afinn}
ipcc_summary <- ipcc_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```



### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 

```{r bind-bing}
ipcc_nrc <- ipcc_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Wait, won't that exclude some of the words in our text? YES! We should check which are excluded using `anti_join()`:

```{r check-exclusions}
ipcc_exclude <- ipcc_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(ipcc_exclude)

# Count to find the most excluded:
ipcc_exclude_n <- ipcc_exclude %>% 
  count(word, sort = TRUE)

head(ipcc_exclude_n)
```

**Lesson: always check which words are EXCLUDED in sentiment analysis using a pre-built lexicon! **

Now find some counts: 
```{r count-bing}
ipcc_nrc_n <- ipcc_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = ipcc_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```

Or count by sentiment *and* word, then facet:
```{r count-nrc}
ipcc_nrc_n5 <- ipcc_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

ipcc_nrc_gg <- ggplot(data = ipcc_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
ipcc_nrc_gg

# Save it
ggsave(plot = ipcc_nrc_gg, 
       here("figures","ipcc_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```

# the word 'lord'
What we see happening with the word 'lord' is what we also saw with the word 'confidence' in the ppcc exersize 
```{r nrc-lord}
conf <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "lord")

# Yep, check it out:
conf
```


## Your task

Taking this script as a point of departure, apply sentiment analysis on the Game of Thrones. You will find a pdf in the data folder. What are the most common meaningful words and what emotions do you expect will dominate this volume? Are there any terms that are similarly ambiguous to the 'confidence' above? 
The word 'lord' is ambiguous, as it is both poitive and negative.

### Credits: 
This tutorial is inspired by Allison Horst's Advanced Statistics and Data Analysis.
